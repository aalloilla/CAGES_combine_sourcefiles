{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38d4aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyodbc\n",
    "import csv\n",
    "import pyworms\n",
    "\n",
    "# DEBUG\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(\"ignore\", category=UserWarning, message='.*')\n",
    "\n",
    "# Output file path\n",
    "outdir = \"D:\\\\00-GCOOS\\\\00-MBON\\\\CAGES\\\\FL\\\\data\\\\\"\n",
    "\n",
    "# THE ORIGINAL CAGES DATABASE FILE is served via WAF but cannot be queried from there\n",
    "# -> download to local HD\n",
    "#db_file = 'https://gcoos4.geos.tamu.edu/WAF/MBON/CAGES/CAGES.accdb'\n",
    "db_file = \"D:\\\\00-GCOOS\\\\00-MBON\\\\CAGES_ORIG\\CAGES.accdb\"\n",
    "user = 'user'\n",
    "password = 'pw'\n",
    "# open the database connection:\n",
    "cnxn = pyodbc.connect('DRIVER={{Microsoft Access Driver (*.mdb, *.accdb)}};DBQ={};Uid={};Pwd={};'.format(db_file, user, password))\n",
    "\n",
    "### GET DATA TABLES\n",
    "\n",
    "# Get joined Stations and Physical tables\n",
    "# --\n",
    "query = 'SELECT st.*, ph.[Start Depth], ph.[End Depth], ph.[Distance Towed], ph.[Secchi Depth], ph.[Soak Time] FROM \"Florida Stations Fixed\" AS st LEFT JOIN \"Florida Physical\" AS ph ON \\\n",
    "st.[Station Code] = ph.[Station Code]'\n",
    "st_ph = pd.read_sql(query, cnxn)\n",
    "# Soak time got converted to datetime with year and all... Source just had hours:minutes, with hours as zero. \n",
    "# -> First drop lines where soak time not given \n",
    "st_ph.drop(st_ph[st_ph['Soak Time'].isna()].index, inplace=True)\n",
    "# -> Get just minutes\n",
    "st_ph['TowTime'] = st_ph['Soak Time'].dt.minute.astype('Int64')\n",
    "# Drop Soak Time \n",
    "st_ph.drop(columns=['Soak Time'], inplace=True)\n",
    "\n",
    "# Get Hydrological data\n",
    "# --\n",
    "query = 'SELECT * FROM \"Florida Hydrological\"'\n",
    "hydro = pd.read_sql(query, cnxn)\n",
    "# Get lowest depth (surface) and deepest depth (bottom) values.\n",
    "# In source data they are in order from smallest to biggest depth -> use first, last\n",
    "hydro_surface = hydro.groupby('Station Code').agg({'Depth': 'first', 'Temperature': 'first', 'Conductivity': 'first', 'pH': 'first', 'Salinity': 'first', 'DO': 'first'}).reset_index()\n",
    "hydro_bottom = hydro.groupby('Station Code').agg({'Depth': 'last', 'Temperature': 'last', 'Conductivity': 'last', 'pH': 'last', 'Salinity': 'last', 'DO': 'last'}).reset_index()\n",
    "# Rename lowest depth/surface (Depth1) and deepest depth/bottom (Depth2) columns for merging\n",
    "for acol in hydro_surface.columns:\n",
    "    if 'Station' in acol:\n",
    "        continue\n",
    "    else:\n",
    "        hydro_surface = hydro_surface.rename(columns={acol: acol + '1'})\n",
    "for acol in hydro_bottom.columns:\n",
    "    if 'Station' in acol:\n",
    "        continue\n",
    "    else:\n",
    "        hydro_bottom = hydro_bottom.rename(columns={acol: acol + '2'})\n",
    "\n",
    "# MERGE surface and bottom values ...\n",
    "merged_hydro = pd.merge(hydro_surface, hydro_bottom, on=['Station Code'])\n",
    "\n",
    "# Merge the hydro data to the stations + physical data\n",
    "st_ph = pd.merge(st_ph, merged_hydro, on=['Station Code'],how='left')\n",
    "st_ph.rename(columns={'Bay code': 'Bay Code'}, inplace=True)\n",
    "\n",
    "# Get joined CPUE and Species tables\n",
    "query = 'SELECT cp.*, sp.[Scientific Name], sp.[Common Name] FROM \"Florida CPUE Fixed\" AS cp LEFT JOIN \"Florida Species\" AS sp ON \\\n",
    "cp.[Species Code] = sp.[Species Code]'\n",
    "cpue_specs = pd.read_sql(query, cnxn)\n",
    "\n",
    "# Convert ScientificName from NoneType to string:\n",
    "cpue_specs.loc[:, 'ScientificName'] = cpue_specs['Scientific Name'].astype('str')\n",
    "# Convert Species Code from float to integer:\n",
    "cpue_specs.loc[:, 'Species Code'] = cpue_specs['Species Code'].astype('Int64')\n",
    "# Drop the rows where the scientific name not known (Missing in the source files)\n",
    "cpue_specs.drop(cpue_specs[cpue_specs['ScientificName'] == 'None'].index, inplace=True)\n",
    "# drop 'Scientific Name'\n",
    "cpue_specs.drop('Scientific Name', axis=1, inplace=True)\n",
    "\n",
    "# merge cpue_specs and stations + physical + hydrological\n",
    "cpue_specs = pd.merge(cpue_specs, st_ph, on=['Station Code','YYYY','MM','DD','Bay Code'],how='left')\n",
    "#cpue_specs.loc['TowTime']\n",
    "\n",
    "# Trawls and Lengths\n",
    "# --\n",
    "query = 'SELECT tr.[Station Code], tr.[Species Code], tr.[Number] AS TotalNumber, len.[Number] AS NumberMeasured, len.[Length] \\\n",
    "FROM \"Florida Trawls\" AS tr LEFT JOIN \"Florida Lengths\" AS len ON tr.[Station Code] = len.[Station Code] AND tr.[Species Code] = len.[Species]'\n",
    "tr_len = pd.read_sql(query, cnxn)\n",
    "\n",
    "intcols = ['Species Code', 'NumberMeasured', 'Length']\n",
    "for acol in intcols:\n",
    "    tr_len.loc[:, acol] = tr_len[acol].astype('Int64')\n",
    "\n",
    "# MERGE cpue_specs and tr_len\n",
    "df = pd.merge(cpue_specs, tr_len, on=['Station Code','Species Code'],how='left')\n",
    "\n",
    "\n",
    "# Get rid of spaces in column names\n",
    "for acol in df.columns.to_list():\n",
    "    df.rename({acol: acol.replace(\" \",\"\")}, axis=\"columns\", inplace=True)\n",
    "\n",
    "#Convert StationCode from NoneType to string\n",
    "df.loc[:, 'StationCode'] = df['StationCode'].astype('Int64')\n",
    "\n",
    "# Generate Datetime field\n",
    "df['datestr'] = df['YYYY'].astype(str) + '-' + df['MM'].astype(str) + '-' + df['DD'].astype(str) + ' 12:00'\n",
    "df['Datetime'] = pd.to_datetime(df['datestr'], utc=True)\n",
    "\n",
    "# drop columns that are not needed for output\n",
    "df.drop(['datestr', 'YYYY', 'MM', 'DD'], axis=1, inplace=True)\n",
    "\n",
    "# sort by date\n",
    "df.sort_values(['Datetime'], axis=0, ascending=True, inplace=True, ignore_index=True)\n",
    "# Format datetime string\n",
    "df['Datetime'] = df['Datetime'].dt.strftime('%Y-%m-%dT%H:%MZ')\n",
    "\n",
    "# Write merged data out to a .csv file\n",
    "cols_out = ['Datetime','Latitude','Longitude','StationCode','BayCode','SpeciesCode','cpue','ScientificName','CommonName','ReferenceCode','TotalNumber','NumberMeasured','Length','StartDepth','EndDepth','DistanceTowed','TowTime','SecchiDepth','Depth1','Temperature1','Conductivity1','pH1','Salinity1','DO1','Depth2','Temperature2','Conductivity2','pH2','Salinity2','DO2']\n",
    "df[cols_out].to_csv(outdir + \"CAGES_CPUE_FL.csv\", encoding='utf-8', index=False)\n",
    "\n",
    "# Close the database connection\n",
    "cnxn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f26e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate ERDDAP datasets .XML -snippet using templates according to variable type\n",
    "\n",
    "## Paths\n",
    "# ouput\n",
    "xml_output = outdir + \"CAGES_FL_XML.txt\"\n",
    "# template files\n",
    "xml_header_temp = \"D:\\\\00-GCOOS\\\\00-MBON\\\\CAGES\\\\erdxml\\\\\" + \"cages_header_template_FL.txt\"\n",
    "xml_int_temp = \"D:\\\\00-GCOOS\\\\00-MBON\\\\CAGES\\\\erdxml\\\\\" + \"int_xml_template.txt\"\n",
    "xml_float_temp = \"D:\\\\00-GCOOS\\\\00-MBON\\\\CAGES\\\\erdxml\\\\\" + \"float_xml_template.txt\"\n",
    "xml_string_temp = \"D:\\\\00-GCOOS\\\\00-MBON\\\\CAGES\\\\erdxml\\\\\" + \"string_xml_template.txt\"\n",
    "\n",
    "# get types\n",
    "dt = df.dtypes.to_dict()\n",
    "int_cols = []\n",
    "float_cols = []\n",
    "string_cols = []\n",
    "\n",
    "# list types\n",
    "for akey in dt.keys():\n",
    "     if 'nt64' in str(dt[akey]):\n",
    "        int_cols.append(akey)\n",
    "     elif 'float' in str(dt[akey]):\n",
    "         float_cols.append(akey)\n",
    "     elif 'obj' in str(dt[akey]) or 'str' in str(dt[akey]):\n",
    "         string_cols.append(akey)\n",
    "\n",
    "# These belong to the header snippet (no need to generate separately):\n",
    "header_vars = ['Datetime', 'Latitude', 'Longitude']\n",
    "\n",
    "# START concatenating the tamplate snippets together\n",
    "# --\n",
    "\n",
    "# 1st, write the header to the output file\n",
    "with open(xml_output, \"w\") as output_file:\n",
    "    with open(xml_header_temp, \"r\") as file:\n",
    "        output_file.write(file.read())\n",
    "\n",
    "# Loop through the rest\n",
    "for acol in cols_out:\n",
    "    isPhysicalMeasurement = False\n",
    "    if acol in header_vars:\n",
    "        continue\n",
    "    elif acol in int_cols:\n",
    "        template_file = xml_int_temp\n",
    "    elif acol in float_cols:\n",
    "        template_file = xml_float_temp\n",
    "        isPhysicalMeasurement = True\n",
    "    else:\n",
    "        template_file = xml_string_temp\n",
    "    \n",
    "    # Open the xml template file for reading    \n",
    "    with open(template_file, 'r') as tempfile:\n",
    "        # Read the contents of the file\n",
    "        contents = tempfile.read()\n",
    "        # Modify the contents as needed\n",
    "        mod_contents = contents.replace('_VARNAME_', acol)\n",
    "        \n",
    "        # Try to figure out the units for some\n",
    "        if isPhysicalMeasurement:\n",
    "            if 'temperature' in acol.lower():\n",
    "                mod_contents = mod_contents.replace('_UNITS_', 'degree_Celcius')\n",
    "                mod_contents = mod_contents.replace('_CATEGORY_', 'Temperature')\n",
    "                if 'air' in acol.lower():\n",
    "                    mod_contents = mod_contents.replace('_DESTNAME_', 'air_temperature')\n",
    "                elif 'surface' in acol.lower():\n",
    "                    mod_contents = mod_contents.replace('_DESTNAME_', 'sea_surface_temperature')\n",
    "                elif 'bottom' in acol.lower():\n",
    "                    mod_contents = mod_contents.replace('_DESTNAME_', 'sea_water_temperature_at_sea_floor')\n",
    "                else:\n",
    "                    mod_contents = mod_contents.replace('_DESTNAME_', 'sea_water_temperature')\n",
    "            elif 'turbidity' in acol.lower():\n",
    "                mod_contents = mod_contents.replace('_UNITS_', 'NTU')\n",
    "                mod_contents = mod_contents.replace('_CATEGORY_', 'Turbidity')\n",
    "                mod_contents = mod_contents.replace('_DESTNAME_', 'sea_water_turbidity')\n",
    "            elif 'salinity' in acol.lower():\n",
    "                mod_contents = mod_contents.replace('_UNITS_', 'PSU')\n",
    "                mod_contents = mod_contents.replace('_CATEGORY_', 'Salinity')\n",
    "                if 'surface' in acol.lower():\n",
    "                    mod_contents = mod_contents.replace('_DESTNAME_', 'sea_surface_salinity')\n",
    "                elif 'bottom' in acol.lower():\n",
    "                    mod_contents = mod_contents.replace('_DESTNAME_', 'sea_water_salinity_at_sea_floor')\n",
    "                else:\n",
    "                    mod_contents = mod_contents.replace('_DESTNAME_', 'sea_water_salinity')\n",
    "            #sea_water_electrical_conductivity\n",
    "            elif 'conductivity' in acol.lower():\n",
    "                mod_contents = mod_contents.replace('_UNITS_', 'S m-1')\n",
    "                mod_contents = mod_contents.replace('_CATEGORY_', 'Conductivity')\n",
    "                mod_contents = mod_contents.replace('_DESTNAME_', 'sea_water_electrical_conductivity')\n",
    "            elif 'DO' in acol or 'dissolved' in acol.lower() or 'oxygen' in acol.lower():\n",
    "                mod_contents = mod_contents.replace('_UNITS_', 'mg l-1')\n",
    "                mod_contents = mod_contents.replace('_CATEGORY_', 'Dissolved O2')\n",
    "                mod_contents = mod_contents.replace('_DESTNAME_', 'mass_concentration_of_oxygen_in_sea_water')\n",
    "            else:\n",
    "                mod_contents = mod_contents.replace('_DESTNAME_',acol)\n",
    "\n",
    "    # Open the ouput file for appending\n",
    "    with open(xml_output, 'a') as file2:\n",
    "        # Write the modified contents back to the file\n",
    "        file2.write(mod_contents)\n",
    "\n",
    "### Add the closing tag to the output xml file:\n",
    "with open(xml_output, 'a') as output_file:\n",
    "    output_file.write(\"</dataset>\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ioos)",
   "language": "python",
   "name": "ioos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
