{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3328c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import pyworms\n",
    "\n",
    "indir = 'https://gcoos4.geos.tamu.edu/WAF/MBON/CAGES/TX/raw_data/'\n",
    "#indir = 'D:\\\\00-GCOOS\\\\00-MBON\\\\CAGES\\\\TX\\\\data\\\\raw\\\\'\n",
    "\n",
    "# The path separator for the input data (local/web/Windows/Linux differ)\n",
    "sep = '/'  #sep = '\\\\'\n",
    "\n",
    "outdir = 'D:\\\\00-GCOOS\\\\00-MBON\\\\CAGES\\\\TX\\\\data\\\\merged\\\\'\n",
    "dbgdir = 'D:\\\\00-GCOOS\\\\00-MBON\\\\CAGES\\\\TX\\\\data\\\\debug\\\\'\n",
    "\n",
    "# Define range of years to process\n",
    "firstyear = 1982\n",
    "lastyear = 2005\n",
    "\n",
    "# Define static input files \n",
    "station_file = indir + 'CAGES_Texas_Stations_f408_c675_c3df.csv'\n",
    "subbay_file = indir + 'CAGES_Texas_SubBays_b8ce_c104_0861.csv'\n",
    "hydro_file = indir + 'CAGES_Texas_Hydrological.csv'\n",
    "length_file = indir + 'CAGES_Texas_Lengths_f6a9_dc7f_aff8.csv'\n",
    "trawls_file = indir + 'CAGES_Texas_Trawls_b0be_9212_e952.csv'\n",
    "species_file = indir + 'CAGES_TX_species.csv'\n",
    "#old_species_file = indir + 'CAGES_Texas_species_matched' + '.csv' # this was manually collected, hard to maintain\n",
    "\n",
    "# Read in the common data (non-annual files)\n",
    "station_data = pd.read_csv(station_file)\n",
    "bay_data  = pd.read_csv(subbay_file)\n",
    "hydro_data = pd.read_csv(hydro_file)\n",
    "length_data = pd.read_csv(length_file)\n",
    "trawl_data = pd.read_csv(trawls_file)\n",
    "\n",
    "# Define the output file paths\n",
    "# --\n",
    "# intermediate files for debugging, commented-out ...\n",
    "# fileout1 = outdir + 'Texas_sam_sta' +'.csv'\n",
    "# fileout2 = outdir + 'Texas_sam_sta_bay' +'.csv'\n",
    "# fileout3 = outdir + 'Texas_sam_sta_bay_hyd' +'.csv'\n",
    "# fileout4 = outdir + 'Texas_sam_sta_bay_hyd_cpue' +'.csv'\n",
    "# fileout5 = outdir + 'Texas_sam_sta_bay_hyd_cpue_len' +'.csv'\n",
    "\n",
    "# For debugging missing species taxonomy data:\n",
    "missing_specs = pd.DataFrame(columns=['Species_Code','Scientific_Name','AphiaID','scientificName','scientificNameAuthorship','taxonomicStatus','taxonRank','scientificNameID','valid_AphiaID','acceptedNameUsage','acceptedScientificNameAuthorship','kingdom','phylum','class','order','family','genus','isMarine','isBrackish','isFreshwater','isTerrestrial'])\n",
    "\n",
    "# Loop through all years\n",
    "for ayear in range(firstyear,lastyear+1):\n",
    "    year_str = str(ayear)\n",
    "\n",
    "    # Define the merged and WoRMS taxonomy -added output:\n",
    "    fileout_merged = outdir + 'CAGES_Texas_merged_' + year_str + '.csv'\n",
    "    \n",
    "    print(f'--- Starting {year_str} data ...')\n",
    "    \n",
    "    # Get the annual data\n",
    "    sample_file = indir + year_str + sep + 'CAGES_Texas_Samples_' + year_str + '.csv'\n",
    "    cpue_file = indir + year_str + sep + 'CAGES_Texas_CPUE_' + year_str + '.csv'\n",
    "\n",
    "    # ADDING SAMPLE AND STATION DATA\n",
    "    sample_data = pd.read_csv(sample_file)\n",
    "    data_test0 = pd.merge(sample_data, station_data, on=['Station_Code'], how='inner')\n",
    "    #data_test0.to_csv(fileout1)\n",
    "    del sample_data\n",
    "    # ADDING BAY CODES\n",
    "    data_test1 = pd.merge(data_test0, bay_data, on=['Bay_Code','SubBay_Code'],how='left')\n",
    "    #data_test1.to_csv(fileout2)\n",
    "    del data_test0\n",
    "    # ADDING HYDRO DATA\n",
    "    data_test2 = pd.merge(data_test1, hydro_data, on=['Sample_Code'],how='inner')\n",
    "    #data_test2.to_csv(fileout3)\n",
    "    del data_test1\n",
    "    # ADDING CPUE DATA\n",
    "    cpue_data  = pd.read_csv(cpue_file)\n",
    "    data_test3 = pd.merge(data_test2, cpue_data, on=['Sample_Code','YYYY','MM','DD','Date','Bay_Code'],how='inner')\n",
    "    #data_test3.to_csv(fileout4)\n",
    "    del cpue_data\n",
    "    del data_test2\n",
    "    # ADDING LENGTH DATA\n",
    "    data_test4 = pd.merge(data_test3, length_data, on=['Sample_Code','Species_Code'],how='left')\n",
    "    #data_test4.to_csv(fileout5)\n",
    "    del data_test3\n",
    "    # ADDING TRAWL DATA\n",
    "    data_test5 = pd.merge(data_test4, trawl_data, on=['Sample_Code','Species_Code'],how='left')\n",
    "    # DEBUG: print out intermediate ersult before adding taxonomy\n",
    "    fileout6 = dbgdir + 'Texas_sam_sta_bay_hyd_cpue_len_tr_' + year_str + '.csv'\n",
    "    data_test5.to_csv(fileout6)\n",
    "    del data_test4\n",
    "\n",
    "    # ADDING TAXONOMY\n",
    "    # --\n",
    "    #list of all species\n",
    "    specs = data_test5[['Species_Code','Scientific_Name']].drop_duplicates().copy()\n",
    "\n",
    "    # Query WoRMS for the taxonomic data for each spec, and add those as new columns\n",
    "    for index, row in specs.iterrows():\n",
    "        aspec = row['Scientific_Name']\n",
    "        print(aspec,'...')\n",
    "        # IDEA: could have just the pyworms API call in the try branch. \n",
    "        # Why: because now some species report as failed due to just some fields not populating\n",
    "        # ... should only report fail when the API call fails!\n",
    "        try:\n",
    "            response = pyworms.aphiaRecordsByMatchNames(aspec,marine_only=False)\n",
    "        except:\n",
    "            print(f'--- Warning! {year_str} WoRMS API call for {aspec} failed!')\n",
    "\n",
    "        try:\n",
    "            resp = response[0][0]\n",
    "            specs.loc[index, 'AphiaID'] = pd.Series(resp['AphiaID'], dtype='Int64')[0]\n",
    "            specs.loc[index, 'scientificName'] = resp['scientificname']\n",
    "            specs.loc[index, 'scientificNameAuthorship'] = resp['authority']\n",
    "            specs.loc[index, 'taxonomicStatus'] = resp['status']\n",
    "            specs.loc[index, 'taxonRank'] = resp['rank']\n",
    "            specs.loc[index, 'scientificNameID'] = resp['lsid']\n",
    "\n",
    "            specs.loc[index, 'valid_AphiaID'] = int(resp['valid_AphiaID'])\n",
    "            specs.loc[index, 'acceptedNameUsage'] = resp['valid_name']\n",
    "            specs.loc[index, 'acceptedScientificNameAuthorship'] = resp['valid_authority']\n",
    "\n",
    "            #Kingdom,Phylum,Class,Order,Family,Genus,Subgenus,Species\n",
    "            specs.loc[index, 'kingdom'] = resp['kingdom']\n",
    "            specs.loc[index, 'phylum'] = resp['phylum']\n",
    "            specs.loc[index, 'class'] = resp['class']\n",
    "            specs.loc[index, 'order'] = resp['order']\n",
    "            specs.loc[index, 'family'] = resp['family']\n",
    "            specs.loc[index, 'genus'] = resp['genus']\n",
    "\n",
    "            specs.loc[index, 'isMarine'] = int(resp['isMarine'])\n",
    "            specs.loc[index, 'isBrackish'] = int(resp['isBrackish'])\n",
    "            specs.loc[index, 'isFreshwater'] = int(resp['isFreshwater'])\n",
    "            specs.loc[index, 'isTerrestrial'] = int(resp['isTerrestrial'])\n",
    "        except:\n",
    "            print(f'--- Warning! {year_str} WoRMS API response for {aspec} has issues, check the output!')\n",
    "\n",
    "    # Read datafile for species (manually composed, for species that fail via API)\n",
    "    species_lookup = pd.read_csv(species_file)\n",
    "\n",
    "    # Update missing values in specs with values in species_lookup\n",
    "    # --\n",
    "    # Set 'Species_Code' column as index in both DataFrames\n",
    "    specs = specs.set_index('Species_Code')\n",
    "    species_lookup = species_lookup.set_index('Species_Code')\n",
    "    specs.update(species_lookup, overwrite=False)\n",
    "    specs = specs.reset_index()\n",
    "\n",
    "    # NaN values force integer columns to float, convert back to nullable integer (Int64)\n",
    "    # --\n",
    "    intcols = ['AphiaID', 'valid_AphiaID', 'isMarine', 'isBrackish', 'isFreshwater', 'isTerrestrial']\n",
    "    for acol in intcols:\n",
    "        specs[acol] = specs[acol].astype('Int64')\n",
    "    \n",
    "    # Check that there are no null values! (unaccounted for species)\n",
    "    # Collect troublemaker list to a dataframe\n",
    "    # --\n",
    "    missing_specs_annual = specs[specs['AphiaID'].isnull()]\n",
    "    miss_intcols = ['Species_Code', 'AphiaID', 'valid_AphiaID', 'isMarine', 'isBrackish', 'isFreshwater', 'isTerrestrial']\n",
    "    if len(missing_specs_annual) != 0:\n",
    "        print(f\"--- Error! {year_str} files have issues with taxonomy data.\")\n",
    "        missing_specs = pd.concat([missing_specs, missing_specs_annual], ignore_index=True)\n",
    "        for acol in miss_intcols:\n",
    "            missing_specs[acol] = missing_specs[acol].astype('Int64')\n",
    "        missing_specs.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # Merge the newly collected species data (from Worms via API)\n",
    "    # (also get rid of duplicate 'Scientific_Name' columns)\n",
    "    # --\n",
    "    data_test6 = pd.merge(data_test5.drop(columns=['Scientific_Name']), specs.drop(columns=['Scientific_Name']), on='Species_Code',how='left')\n",
    "\n",
    "    # Write out the merged file, make sure all characters print OK\n",
    "    # --\n",
    "    data_test6.to_csv(fileout_merged, encoding='utf-8', index=False)\n",
    "\n",
    "    # delete loop variables\n",
    "    del specs, species_lookup, intcols, data_test5, data_test6, missing_specs_annual\n",
    "    print(f'--- End year {year_str}.')\n",
    "    # --\n",
    "    ### END FOR\n",
    "                                 \n",
    "# print merged missing species data for debugging:\n",
    "missing_specs.to_csv(dbgdir + 'missing_species.csv')\n",
    "\n",
    "# delete common variables \n",
    "del station_data, bay_data, hydro_data, length_data, trawl_data, missing_specs\n",
    "print('--- Done! ---')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ioos)",
   "language": "python",
   "name": "ioos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
