{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49739de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyodbc\n",
    "import csv\n",
    "import pyworms\n",
    "\n",
    "# DEBUG\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(\"ignore\", category=UserWarning, message='.*')\n",
    "\n",
    "# Output file path\n",
    "outdir = \"D:\\\\00-GCOOS\\\\00-MBON\\\\CAGES\\\\AL\\\\data\\\\\"\n",
    "\n",
    "# THE ORIGINAL CAGES DATABASE FILE is served via WAF but cannot be queried from there\n",
    "# -> download to local HD\n",
    "#db_file = 'https://gcoos4.geos.tamu.edu/WAF/MBON/CAGES/CAGES.accdb'\n",
    "db_file = \"D:\\\\00-GCOOS\\\\00-MBON\\\\CAGES_ORIG\\CAGES.accdb\"\n",
    "user = 'user'\n",
    "password = 'pw'\n",
    "# open the database connection:\n",
    "cnxn = pyodbc.connect('DRIVER={{Microsoft Access Driver (*.mdb, *.accdb)}};DBQ={};Uid={};Pwd={};'.format(db_file, user, password))\n",
    "\n",
    "### GET DATA TABLES\n",
    "\n",
    "# -- CPUE and Stations\n",
    "query = 'SELECT * FROM \"Alabama CPUE\" AS cpue LEFT JOIN \"Alabama Stations\" AS stations ON \\\n",
    "cpue.[Station Code] = stations.[Station Code]'\n",
    "cpuedata = pd.read_sql(query, cnxn)\n",
    "# Remove duplicate columns\n",
    "cpuedata = cpuedata.loc[:, ~cpuedata.columns.duplicated(keep='first')]\n",
    "\n",
    "# -- TRAWLS and SPECIES\n",
    "query = 'SELECT * FROM \"Alabama Trawls\" AS trawls LEFT JOIN \"Alabama Species\" AS species ON \\\n",
    "trawls.[Species Code] = species.[Species Code]'\n",
    "trawlsdata = pd.read_sql(query, cnxn)\n",
    "\n",
    "# Remove duplicate columns\n",
    "trawlsdata = trawlsdata.loc[:, ~trawlsdata.columns.duplicated(keep='first')]\n",
    "\n",
    "# Convert Species Code back to integer\n",
    "intcols = ['Species Code']\n",
    "for acol in intcols:\n",
    "    #trawlsdata[acol] = trawlsdata[acol].astype('Int64')\n",
    "    trawlsdata.loc[:, acol] = trawlsdata[acol].astype('Int64')\n",
    "    cpuedata.loc[:, acol] = cpuedata[acol].astype('Int64')\n",
    "# Drop outdated original data columns (up to date values will be extracted at DarwinCore alignment stage via the WoRMS API)\n",
    "dropcols = ['Phylum','Class','Family','Common Name','Prior Name']\n",
    "for acol in dropcols:\n",
    "    trawlsdata.drop(acol, axis=1, inplace=True)\n",
    "\n",
    "# Merge CPUE and Trawls data\n",
    "df = pd.merge(cpuedata, trawlsdata, on=['Sample Code','Species Code'],how='left')\n",
    "\n",
    "# -- Gear\n",
    "query = 'SELECT * FROM \"Alabama Gear\"'\n",
    "df = pd.merge(df, pd.read_sql(query, cnxn), on=['Gear Code'],how='left')\n",
    "\n",
    "# -- Hydrological\n",
    "query = 'SELECT * FROM \"Alabama Hydrological\"'\n",
    "df = pd.merge(df, pd.read_sql(query, cnxn), on=['Sample Code'],how='left')\n",
    "\n",
    "# -- Lengths\n",
    "query = 'SELECT * FROM \"Alabama Lengths\"'\n",
    "lengths = pd.read_sql(query, cnxn)\n",
    "#df = pd.merge(df, pd.read_sql(query, cnxn), on=['Sample Code','Species Code'],how='left')\n",
    "intcols = ['Species Code']\n",
    "for acol in intcols:\n",
    "    lengths.loc[:, acol] = lengths[acol].astype('Int64')\n",
    "\n",
    "df = pd.merge(df, lengths, on=['Sample Code','Species Code'],how='left')\n",
    "\n",
    "# Close the database connection\n",
    "cnxn.close()\n",
    "\n",
    "# Generate Datetime field\n",
    "df['datestr'] = df['YYYY'].astype(str) + '-' + df['MM'].astype(str) + '-' + df['DD'].astype(str) + ' 12:00'\n",
    "df['Datetime'] = pd.to_datetime(df['datestr'], utc=True)\n",
    "\n",
    "# drop columns that are not needed for output\n",
    "df.drop(['datestr', 'YYYY', 'MM', 'DD'], axis=1, inplace=True)\n",
    "\n",
    "# sort by date\n",
    "df.sort_values(['Datetime'], axis=0, ascending=True, inplace=True, ignore_index=True)\n",
    "# Format datetime string\n",
    "df['Datetime'] = df['Datetime'].dt.strftime('%Y-%m-%dT%H:%MZ')\n",
    "\n",
    "# Get rid of spaces in column names\n",
    "for acol in df.columns.to_list():\n",
    "    df.rename({acol: acol.replace(\" \",\"\")}, axis=\"columns\", inplace=True)\n",
    "\n",
    "# Convert ScientificName from NoneType to string:\n",
    "df.loc[:, 'ScientificName'] = df['ScientificName'].astype('str')\n",
    "# Drop the rows where the scientific name not known (Missing in the source files)\n",
    "df.drop(df[df['ScientificName'] == 'None'].index, inplace=True)\n",
    "\n",
    "### OUTPUT TO FILE\n",
    "cols_out = ['Datetime','Latitude', 'Longitude','SampleCode', 'StationCode','Station','Description','WaterBody','Salinity','Temperature','DO','GearCode','Gear','SpeciesCode','ScientificName','cpue','Measured','TotalNumber','TotalWeight','Commercial','Length']\n",
    "# Write merged data out to a .csv file\n",
    "df[cols_out].to_csv(outdir + \"CAGES_CPUE_AL.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f26e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate ERDDAP datasets .XML -snippet using templates according to variable type\n",
    "\n",
    "## Paths \n",
    "# ouput\n",
    "xml_output = outdir + \"CAGES_AL_XML.txt\"\n",
    "# template files\n",
    "xml_header_temp = \"D:\\\\00-GCOOS\\\\00-MBON\\\\CAGES\\\\erdxml\\\\\" + \"cages_header_template_AL.txt\"\n",
    "xml_int_temp = \"D:\\\\00-GCOOS\\\\00-MBON\\\\CAGES\\\\erdxml\\\\\" + \"int_xml_template.txt\"\n",
    "xml_float_temp = \"D:\\\\00-GCOOS\\\\00-MBON\\\\CAGES\\\\erdxml\\\\\" + \"float_xml_template.txt\"\n",
    "xml_string_temp = \"D:\\\\00-GCOOS\\\\00-MBON\\\\CAGES\\\\erdxml\\\\\" + \"string_xml_template.txt\"\n",
    "\n",
    "# get types\n",
    "dt = df.dtypes.to_dict()\n",
    "int_cols = []\n",
    "float_cols = []\n",
    "string_cols = []\n",
    "\n",
    "# list types\n",
    "for akey in dt.keys():\n",
    "     if 'nt64' in str(dt[akey]):\n",
    "        int_cols.append(akey)\n",
    "     elif 'float' in str(dt[akey]):\n",
    "         float_cols.append(akey)\n",
    "     elif 'obj' in str(dt[akey]) or 'str' in str(dt[akey]):\n",
    "         string_cols.append(akey)\n",
    "\n",
    "# These belong to the header snippet (no need to generate separately):\n",
    "header_vars = ['Datetime', 'Latitude', 'Longitude']\n",
    "\n",
    "# START concatenating the tamplate snippets together\n",
    "# --\n",
    "\n",
    "# 1st, write the header to the output file\n",
    "with open(xml_output, \"w\") as output_file:\n",
    "    with open(xml_header_temp, \"r\") as file:\n",
    "        output_file.write(file.read())\n",
    "\n",
    "# Loop throgh the rest\n",
    "for acol in cols_out:\n",
    "    isPhysicalMeasurement = False\n",
    "    if acol in header_vars:\n",
    "        continue\n",
    "    elif acol in int_cols:\n",
    "        template_file = xml_int_temp\n",
    "    elif acol in float_cols:\n",
    "        template_file = xml_float_temp\n",
    "        isPhysicalMeasurement = True\n",
    "    else:\n",
    "        template_file = xml_string_temp\n",
    "    \n",
    "    # Open the xml template file for reading    \n",
    "    with open(template_file, 'r') as tempfile:\n",
    "        # Read the contents of the file\n",
    "        contents = tempfile.read()\n",
    "        # Modify the contents as needed\n",
    "        mod_contents = contents.replace('_VARNAME_', acol)\n",
    "        \n",
    "        # Try to figure out the units for some\n",
    "        if isPhysicalMeasurement:\n",
    "            if 'temperature' in acol.lower():\n",
    "                mod_contents = mod_contents.replace('_UNITS_', 'degree_Celcius')\n",
    "                mod_contents = mod_contents.replace('_CATEGORY_', 'Temperature')\n",
    "                if 'air' in acol.lower():\n",
    "                    mod_contents = mod_contents.replace('_DESTNAME_', 'air_temperature')\n",
    "                elif 'surface' in acol.lower():\n",
    "                    mod_contents = mod_contents.replace('_DESTNAME_', 'sea_surface_temperature')\n",
    "                elif 'bottom' in acol.lower():\n",
    "                    mod_contents = mod_contents.replace('_DESTNAME_', 'sea_water_temperature_at_sea_floor')\n",
    "                else:\n",
    "                    mod_contents = mod_contents.replace('_DESTNAME_', 'sea_water_temperature')\n",
    "            elif 'turbidity' in acol.lower():\n",
    "                mod_contents = mod_contents.replace('_UNITS_', 'NTU')\n",
    "                mod_contents = mod_contents.replace('_CATEGORY_', 'Turbidity')\n",
    "                mod_contents = mod_contents.replace('_DESTNAME_', 'sea_water_turbidity')\n",
    "            elif 'salinity' in acol.lower():\n",
    "                mod_contents = mod_contents.replace('_UNITS_', 'PSU')\n",
    "                mod_contents = mod_contents.replace('_CATEGORY_', 'Salinity')\n",
    "                if 'surface' in acol.lower():\n",
    "                    mod_contents = mod_contents.replace('_DESTNAME_', 'sea_surface_salinity')\n",
    "                elif 'bottom' in acol.lower():\n",
    "                    mod_contents = mod_contents.replace('_DESTNAME_', 'sea_water_salinity_at_sea_floor')\n",
    "                else:\n",
    "                    mod_contents = mod_contents.replace('_DESTNAME_', 'sea_water_salinity')\n",
    "            elif 'DO' in acol or 'dissolved' in acol.lower() or 'oxygen' in acol.lower():\n",
    "                mod_contents = mod_contents.replace('_UNITS_', 'mg l-1')\n",
    "                mod_contents = mod_contents.replace('_CATEGORY_', 'Dissolved O2')\n",
    "                mod_contents = mod_contents.replace('_DESTNAME_', 'mass_concentration_of_oxygen_in_sea_water')\n",
    "            else:\n",
    "                mod_contents = mod_contents.replace('_DESTNAME_',acol)\n",
    "\n",
    "    # Open the ouput file for appending\n",
    "    with open(xml_output, 'a') as file2:\n",
    "        # Write the modified contents back to the file\n",
    "        file2.write(mod_contents)\n",
    "\n",
    "### Add the closing tag to the output xml file:\n",
    "with open(xml_output, 'a') as output_file:\n",
    "    output_file.write(\"</dataset>\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ioos)",
   "language": "python",
   "name": "ioos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
